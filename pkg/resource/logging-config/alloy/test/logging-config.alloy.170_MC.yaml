# This file was generated by logging-operator.
# It configures Alloy to be used as a logging agent.
# - configMap is generated from logging.alloy.template and passed as a string
#   here and will be created by Alloy's chart.
# - Alloy runs as a daemonset, with required tolerations in order to scrape logs
#   from every machine in the cluster.
# - Running as root user is required in order to be able to read log files within
#   /var/log/journal and /run/log/journal directories.
# - NODENAME env var is used as additional label for kubernetes_audit logs.
networkPolicy:
  cilium:
    egress:
    - toEntities:
      - kube-apiserver
      - world
    - toEndpoints:
      - matchLabels:
          io.kubernetes.pod.namespace: kube-system
          k8s-app: coredns
      - matchLabels:
          io.kubernetes.pod.namespace: kube-system
          k8s-app: k8s-dns-node-cache
      toPorts:
      - ports:
        - port: "1053"
          protocol: UDP
        - port: "1053"
          protocol: TCP
        - port: "53"
          protocol: UDP
        - port: "53"
          protocol: TCP
    # Allow direct access to loki-backend and nginx
    - toEndpoints:
      - matchLabels:
          app.kubernetes.io/component: backend
          app.kubernetes.io/name: loki
          io.kubernetes.pod.namespace: loki
      toPorts:
      - ports:
        - port: "3100"
          protocol: TCP
    - toEndpoints:
      - matchLabels:
          app.kubernetes.io/name: ingress-nginx
      toPorts:
      - ports:
        - port: "80"
          protocol: ANY
        - port: "443"
          protocol: ANY
    # Allow clustering
    - toEndpoints:
      - matchLabels:
          app.kubernetes.io/instance: alloy-logs
          app.kubernetes.io/name: alloy
      toPorts:
      - ports:
        - port: "12345"
          protocol: TCP
  endpointSelector:
    matchLabels:
      app.kubernetes.io/instance: alloy-logs
      app.kubernetes.io/name: alloy

alloy:
  alloy:
    configMap:
      create: true
      content: |-
        logging {
        	level  = "warn"
        	format = "logfmt"
        }
        
        remote.kubernetes.secret "credentials" {
        	namespace = "kube-system"
        	name = "alloy-logs"
        }
        // load rules for tenant giantswarm
        loki.rules.kubernetes "giantswarm" {
        	address = "http://loki-backend.loki.svc:3100/"
        	loki_namespace_prefix = "test-installation"
        	tenant_id = "giantswarm"
        	rule_selector {
        		match_labels = {
        			"observability.giantswarm.io/tenant" = "giantswarm",
        		}
        		match_expression {
        			key = "application.giantswarm.io/prometheus-rule-kind"
        			operator = "In"
        			values = ["loki"]
        		}
        	}
        }
        
        // Kubernetes pods logs
        loki.source.podlogs "kubernetes_pods" {
        	forward_to      = [loki.relabel.kubernetes_pods.receiver]
        	// namespace_selector is a workaround for broken cluster mode in alloy 1.5.0
        	namespace_selector {
        		match_expression {
        			key = "nonexisting"
        			operator = "NotIn"
        			values = ["nonexistant"]
        		}
        	}
        	clustering {
        		enabled = true
        	}
        }
        
        loki.relabel "kubernetes_pods" {
        	forward_to      = [loki.process.kubernetes_pods.receiver]
        
        	rule {
        		source_labels = ["__meta_kubernetes_pod_controller_name"]
        		regex         = "([0-9a-z-.]+?)(-[0-9a-f]{8,10})?"
        		target_label  = "__tmp_controller_name"
        	}
        
        	rule {
        		source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name", "__meta_kubernetes_pod_label_app", "__tmp_controller_name", "__meta_kubernetes_pod_name"]
        		regex         = "^;*([^;]+)(;.*)?$"
        		target_label  = "app"
        	}
        
        	rule {
        		source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_instance", "__meta_kubernetes_pod_label_instance"]
        		regex         = "^;*([^;]+)(;.*)?$"
        		target_label  = "instance"
        	}
        
        	rule {
        		source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_component", "__meta_kubernetes_pod_label_component"]
        		regex         = "^;*([^;]+)(;.*)?$"
        		target_label  = "component"
        	}
        
        	rule {
        		target_label = "scrape_job"
        		replacement  = "kubernetes-pods"
        	}
        
        	rule {
        		source_labels = ["__meta_kubernetes_pod_node_name"]
        		target_label  = "node"
        	}
        	rule {
        		source_labels = ["instance"]
        		regex         = "(.+)/.+"
        		target_label  = "namespace"
        	}
        
        	rule {
        		source_labels = ["instance"]
        		regex         = ".+/(.+):.+"
        		target_label  = "pod"
        	}
        
        	rule {
        		source_labels = ["instance"]
        		regex         = ".+/.+:(.+)"
        		target_label  = "container"
        	}
        	// Keep only log lines with tenants specified in the regex.
        	// Log lines are dropped later in the loki.process stage.drop step.
        	rule {
        		source_labels = ["giantswarm_observability_tenant"]
        		regex         = "^(giantswarm)$"
        		target_label  = "__tenant_id__"
        	}
        
        	// We drop the label to not have it ingested in loki
        	rule {
        		regex = "giantswarm_observability_tenant"
        		action = "labeldrop"
        	}
        }
        
        loki.process "kubernetes_pods" {
        	forward_to = [loki.write.default.receiver]
        
        	stage.cri { }
        	// Drop log lines that do not contain a tenant id
        	stage.drop {
        		drop_counter_reason = "no_tenant_id"
        		source              = "__tenant_id__"
        		expression          = "^$"
        	}
        
        	stage.structured_metadata {
        		values = {
        			"filename" = "",
        			"stream" = "",
        		}
        	}
        
        	stage.label_drop {
        		values = [
        			"filename",
        			"stream",
        		]
        	}
        }
        
        // journald logs from /run/log/journal
        loki.process "systemd_journal_run" {
        	forward_to = [loki.write.default.receiver]
        
        	stage.json {
        		expressions = {
        			SYSLOG_IDENTIFIER = "SYSLOG_IDENTIFIER",
        		}
        	}
        
        	stage.drop {
        		source = "SYSLOG_IDENTIFIER"
        		value  = "audit"
        	}
        }
        
        discovery.relabel "systemd_journal_run" {
        	targets = []
        
        	rule {
        		source_labels = ["__journal__systemd_unit"]
        		target_label  = "__tmp_systemd_unit"
        	}
        
        	rule {
        		source_labels = ["__journal__systemd_unit", "__journal_syslog_identifier"]
        		regex         = ";(.+)"
        		target_label  = "__tmp_systemd_unit"
        	}
        
        	rule {
        		source_labels = ["__tmp_systemd_unit"]
        		target_label  = "systemd_unit"
        	}
        
        	rule {
        		source_labels = ["__journal__hostname"]
        		target_label  = "node"
        	}
        }
        
        loki.source.journal "systemd_journal_run" {
        	format_as_json = true
        	max_age        = "12h0m0s"
        	path           = "/run/log/journal"
        	relabel_rules  = discovery.relabel.systemd_journal_run.rules
        	forward_to     = [loki.process.systemd_journal_run.receiver]
        	labels         = {
        		scrape_job = "system-logs",
        	}
        }
        
        // journald logs from /var/log/journal
        loki.process "systemd_journal_var" {
        	forward_to = [loki.write.default.receiver]
        
        	stage.json {
        		expressions = {
        			SYSLOG_IDENTIFIER = "SYSLOG_IDENTIFIER",
        		}
        	}
        
        	stage.drop {
        		source = "SYSLOG_IDENTIFIER"
        		value  = "audit"
        	}
        }
        
        discovery.relabel "systemd_journal_var" {
        	targets = []
        
        	rule {
        		source_labels = ["__journal__systemd_unit"]
        		target_label  = "__tmp_systemd_unit"
        	}
        
        	rule {
        		source_labels = ["__journal__systemd_unit", "__journal_syslog_identifier"]
        		regex         = ";(.+)"
        		target_label  = "__tmp_systemd_unit"
        	}
        
        	rule {
        		source_labels = ["__tmp_systemd_unit"]
        		target_label  = "systemd_unit"
        	}
        
        	rule {
        		source_labels = ["__journal__hostname"]
        		target_label  = "node"
        	}
        }
        
        loki.source.journal "systemd_journal_var" {
        	format_as_json = true
        	max_age        = "12h0m0s"
        	path           = "/var/log/journal"
        	relabel_rules  = discovery.relabel.systemd_journal_var.rules
        	forward_to     = [loki.process.systemd_journal_var.receiver]
        	labels         = {
        		scrape_job = "system-logs",
        	}
        }
        
        // Kubernetes API server audit logs
        local.file_match "kubernetes_audit" {
        	path_targets = [{
        		__address__ = "localhost",
        		__path__    = "/var/log/apiserver/audit.log",
        		node   = coalesce(env("NODENAME"), "unknown"),
        		scrape_job  = "audit-logs",
        	}]
        }
        
        loki.process "kubernetes_audit" {
        	forward_to = [loki.write.default.receiver]
        
        	stage.json {
        		expressions = {
        			objectRef = "objectRef",
        		}
        	}
        
        	stage.json {
        		expressions = {
        			namespace = "namespace",
        			resource  = "resource",
        		}
        		source = "objectRef"
        	}
        
        	stage.structured_metadata {
        		values = {
        			"resource" = "",
        			"filename" = "",
        		}
        	}
        
        	stage.label_drop {
        		values = [
        			"filename",
        		]
        	}
        
        	stage.labels {
        		values = {
        			namespace = "",
        		}
        	}
        }
        
        loki.source.file "kubernetes_audit" {
        	targets               = local.file_match.kubernetes_audit.targets
        	forward_to            = [loki.process.kubernetes_audit.receiver]
        	legacy_positions_file = "/run/promtail/positions.yaml"
        }
        
        // Loki target configuration
        loki.write "default" {
        	endpoint {
        		url                = nonsensitive(remote.kubernetes.secret.credentials.data["logging-url"])
        		max_backoff_period = "10m"
        		tenant_id          = nonsensitive(remote.kubernetes.secret.credentials.data["logging-tenant-id"])
        
        		basic_auth {
        			username = nonsensitive(remote.kubernetes.secret.credentials.data["logging-username"])
        			password = remote.kubernetes.secret.credentials.data["logging-password"]
        		}
        
        		tls_config {
        			insecure_skip_verify = false
        		}
        	}
        	external_labels = {
        		cluster_id   = "test-installation",
        		installation = "test-installation",
        	}
        }
        
    clustering:
      enabled: true
      name: alloy-logs
    extraEnv:
    - name: NODENAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    mounts:
      varlog: true
      dockercontainers: true
      extra:
      - name: runlogjournal
        mountPath: /run/log/journal
        readOnly: true
      # This is needed to allow alloy to create files when using readOnlyRootFilesystem
      - name: alloy-tmp
        mountPath: /tmp/alloy
    # We decided to configure the alloy-logs resources as such after some investigation done https://github.com/giantswarm/giantswarm/issues/32655
    resources:
      limits:
        cpu: 2000m
        memory: 300Mi
      requests:
        cpu: 25m
        memory: 200Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
      runAsUser: 0
      runAsGroup: 0
      runAsNonRoot: false
      seccompProfile:
        type: RuntimeDefault
  controller:
    type: daemonset
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    volumes:
      extra:
      - name: runlogjournal
        hostPath:
          path: /run/log/journal
      - name: alloy-tmp
        emptyDir: {}
verticalPodAutoscaler:
  enabled: true
  # We decided to configure the alloy-logs vertical pod autoscaler as such after some investigation done https://github.com/giantswarm/giantswarm/issues/32655
  resourcePolicy:
    containerPolicies:
    - containerName: alloy
      controlledResources:
      - memory
      controlledValues: "RequestsAndLimits"
      maxAllowed:
        memory: 1Gi
podLogs:
- name: all-pods
  namespace: kube-system
  spec:
    selector: {}
    namespaceSelector: {}
    relabelings:
    - action: replace
      targetLabel: "giantswarm_observability_tenant"
      replacement: giantswarm
