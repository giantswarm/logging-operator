logging {
	level  = "warn"
	format = "logfmt"
}

remote.kubernetes.secret "credentials" {
	namespace = "kube-system"
	name = "{{ .SecretName }}"
}

{{- range .Tenants }}
// load rules for tenant {{ . }}
loki.rules.kubernetes "{{ . }}" {
	{{- if $.IsWorkloadCluster }}
	address = nonsensitive(remote.kubernetes.secret.credentials.data["{{ $.LokiRulerAPIURLKey }}"])
	basic_auth {
		username = nonsensitive(remote.kubernetes.secret.credentials.data["{{ $.LoggingUsernameKey }}"])
		password = remote.kubernetes.secret.credentials.data["{{ $.LoggingPasswordKey }}"]
	}
	{{- else }}
	address = "http://loki-backend.loki.svc:3100/"
	{{- end }}
	loki_namespace_prefix = "{{ $.ClusterID }}"
	tenant_id = "{{ . }}"
	rule_selector {
		match_labels = {
			"observability.giantswarm.io/tenant" = "{{ . }}",
		}
		match_expression {
			key = "application.giantswarm.io/prometheus-rule-kind"
			operator = "In"
			values = ["loki"]
		}
	}
}
{{- end }}

{{- if .SupportPodLogs }}
// Native podlogs collection (preferred method for scalability)
loki.source.podlogs "kubernetes_pods" {
	forward_to = [loki.relabel.kubernetes_pods.receiver]
	{{- if .NodeFilteringEnabled }}
	node_filter {
		enabled = true
		node_name = env("NODE_NAME")
	}
	{{ else }}
	// Workaround for broken cluster mode in Alloy 1.5.0
	// Uses a non-existent key to effectively select all namespaces
	namespace_selector {
		match_expression {
			key      = "nonexisting"
			operator = "NotIn"
			values   = ["nonexistant"]
		}
	}

	clustering {
		enabled = true
	}
	{{- end }}
}

loki.relabel "kubernetes_pods" {
	forward_to = [loki.process.kubernetes_pods.receiver]
{{- else }}
// Legacy file-based log collection (fallback method)
discovery.kubernetes "kubernetes_pods" {
	role = "pod"
}

local.file_match "kubernetes_pods" {
	path_targets = discovery.relabel.kubernetes_pods.output
}

loki.source.file "kubernetes_pods" {
	targets               = local.file_match.kubernetes_pods.targets
	forward_to            = [loki.process.kubernetes_pods.receiver]
	legacy_positions_file = "/run/alloy/positions.yaml"
}

discovery.relabel "kubernetes_pods" {
	targets = discovery.kubernetes.kubernetes_pods.targets
{{- end }}

	rule {
		target_label = "scrape_job"
		replacement  = "kubernetes-pods"
	}

	{{- if .SupportPodLogs }}
	// Extract namespace, pod, and container from the structured instance label
	// Format: "namespace/pod:container" (e.g., "kube-system/mimir-distributor-abc123:mimir")
	rule {
		source_labels = ["instance"]
		regex         = "([^/]+)/.+"
		target_label  = "namespace"
	}

	rule {
		source_labels = ["instance"]
		regex         = "[^/]+/([^:]+):.+"
		target_label  = "pod"
	}

	rule {
		source_labels = ["instance"]
		regex         = "[^/]+/[^:]+:(.+)"
		target_label  = "container"
	}

	// Extract tenant ID for authorized tenants only - logs from unauthorized
	// tenants will be dropped later in the processing pipeline
	// Configured tenants: {{ join ", " .Tenants }}
	rule {
		source_labels = ["giantswarm_observability_tenant"]
		regex         = "^({{ join "|" .Tenants }})$"
		target_label  = "__tenant_id__"
	}

	// Remove the source tenant label to keep Loki labels clean
	rule {
		regex  = "giantswarm_observability_tenant"
		action = "labeldrop"
	}

	{{- else }}
	rule {
		source_labels = ["__meta_kubernetes_namespace"]
		target_label  = "namespace"
	}

	rule {
		source_labels = ["__meta_kubernetes_pod_name"]
		target_label  = "pod"
	}

	rule {
		source_labels = ["__meta_kubernetes_pod_container_name"]
		target_label  = "container"
	}

	rule {
		source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
		separator     = "/"
		target_label  = "__path__"
		replacement   = "/var/log/pods/*$1/*.log"
	}

	rule {
		source_labels = ["__meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash", "__meta_kubernetes_pod_annotation_kubernetes_io_config_hash", "__meta_kubernetes_pod_container_name"]
		separator     = "/"
		regex         = "true/(.*)"
		target_label  = "__path__"
		replacement   = "/var/log/pods/*$1/*.log"
	}

	rule {
		source_labels = ["__meta_kubernetes_pod_node_name"]
		target_label  = "node"
	}

	// Convert specific Kubernetes pod labels to clean names (only for discovery.kubernetes path)
	rule {
		source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
		target_label  = "app_kubernetes_io_name"
	}

	rule {
		source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_component"]
		target_label  = "app_kubernetes_io_component"
	}

	rule {
		source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_version"]
		target_label  = "app_kubernetes_io_version"
	}

	{{- if .IsWorkloadCluster }}
	// Restrict collection to system namespaces only in workload clusters
	rule {
		source_labels = ["__meta_kubernetes_namespace"]
		regex         = "kube-system|giantswarm"
		action        = "keep"
	}
	{{- end }}
	{{- end }}

	// Extract and normalize standard k8s labels with priority-based fallbacks
	// Priority: app.kubernetes.io/name > app > pod name (pod logs then file-based discovery)
	rule {
		source_labels = ["app_kubernetes_io_name", "app", "pod", "__meta_kubernetes_pod_name"]
		regex         = "^;*([^;]+)(;.*)?$"
		target_label  = "app"
	}

	rule {
		source_labels = ["app_kubernetes_io_component", "component"]
		regex         = "^;*([^;]+)(;.*)?$"
		target_label  = "component"
	}

	rule {
		source_labels = ["app_kubernetes_io_version", "version"]
		regex         = "^;*([^;]+)(;.*)?$"
		target_label  = "version"
	}

	// Create unified service name by combining app + component to align Loki and Tempo signals
	// Only creates service label when BOTH app and component are non-empty
	// Handles app names with hyphens like "alertmanager-to-github" or "background-controller"
	// Examples: "mimir" + "distributor" → "mimir-distributor" (matches Tempo service.name)
	//           "alertmanager-to-github" + "webhook" → "alertmanager-to-github-webhook"
	rule {
		source_labels = ["app", "component"]
		regex         = "^(.+);(.+)$"
		replacement   = "${1}-${2}"
		target_label  = "service"
	}

	rule {
		regex  = "app_kubernetes_io_(component|name|version)"
		action = "labeldrop"
	}

}

loki.process "kubernetes_pods" {
	forward_to = [loki.write.default.receiver]

	// Parse container runtime interface (CRI) log format
	stage.cri { }

	{{- if .SupportPodLogs }}
	// Multi-tenant filtering: drop logs without valid tenant authorization
	stage.drop {
		drop_counter_reason = "no_tenant_id"
		source              = "__tenant_id__"
		expression          = "^$"
	}
	{{- end }}

	// Move high-cardinality metadata to structured metadata instead of labels
	stage.structured_metadata {
		values = {
			"filename" = "",
			"stream" = "",
		}
	}

	// Clean up temporary labels used only for processing
	stage.label_drop {
		values = [
			"filename",
			"stream",
		]
	}
}

// journald logs from /run/log/journal
loki.process "systemd_journal_run" {
	forward_to = [loki.write.default.receiver]

	stage.json {
		expressions = {
			SYSLOG_IDENTIFIER = "SYSLOG_IDENTIFIER",
		}
	}

	stage.drop {
		source = "SYSLOG_IDENTIFIER"
		value  = "audit"
	}
}

discovery.relabel "systemd_journal_run" {
	targets = []

	rule {
		source_labels = ["__journal__systemd_unit"]
		target_label  = "__tmp_systemd_unit"
	}

	rule {
		source_labels = ["__journal__systemd_unit", "__journal_syslog_identifier"]
		regex         = ";(.+)"
		target_label  = "__tmp_systemd_unit"
	}

	rule {
		source_labels = ["__tmp_systemd_unit"]
		target_label  = "systemd_unit"
	}

	rule {
		source_labels = ["__journal__hostname"]
		target_label  = "node"
	}
}

loki.source.journal "systemd_journal_run" {
	format_as_json = true
	max_age        = "12h0m0s"
	path           = "/run/log/journal"
	relabel_rules  = discovery.relabel.systemd_journal_run.rules
	forward_to     = [loki.process.systemd_journal_run.receiver]
	labels         = {
		scrape_job = "system-logs",
	}
}

// Kubernetes API server audit logs
local.file_match "kubernetes_audit" {
	path_targets = [{
		__address__ = "localhost",
		__path__    = "/var/log/apiserver/audit.log",
		node   = coalesce(env("NODENAME"), "unknown"),
		scrape_job  = "audit-logs",
	}]
}

loki.process "kubernetes_audit" {
	forward_to = [loki.write.default.receiver]

	stage.json {
		expressions = {
			objectRef = "objectRef",
		}
	}

	stage.json {
		expressions = {
			namespace = "namespace",
			resource  = "resource",
		}
		source = "objectRef"
	}

	stage.structured_metadata {
		values = {
			"resource" = "",
			"filename" = "",
		}
	}

	stage.label_drop {
		values = [
			"filename",
		]
	}

	stage.labels {
		values = {
			namespace = "",
		}
	}
}

loki.source.file "kubernetes_audit" {
	targets               = local.file_match.kubernetes_audit.targets
	forward_to            = [loki.process.kubernetes_audit.receiver]
	legacy_positions_file = "/run/alloy/positions.yaml"
}

// Loki target configuration
loki.write "default" {
	endpoint {
		{{- if .IsWorkloadCluster }}
		basic_auth {
			username = nonsensitive(remote.kubernetes.secret.credentials.data["{{ .LoggingUsernameKey }}"])
			password = remote.kubernetes.secret.credentials.data["{{ .LoggingPasswordKey }}"]
		}
		url                = nonsensitive(remote.kubernetes.secret.credentials.data["{{ .LoggingURLKey }}"])
		{{- else }}
		url                = "http://loki-gateway.loki.svc:80/loki/api/v1/push"
		{{- end }}
		max_backoff_period = "{{ .MaxBackoffPeriod }}"
		remote_timeout     = "{{ .RemoteTimeout }}"
		tenant_id          = nonsensitive(remote.kubernetes.secret.credentials.data["{{ .LoggingTenantIDKey }}"])

		tls_config {
			insecure_skip_verify = {{ .InsecureSkipVerify }}
		}
	}
	external_labels = {
		cluster_id       = "{{ .ClusterID }}",
		cluster_type     = "{{ .ClusterType }}",
		organization     = "{{ .Organization }}",
		provider         = "{{ .Provider }}",
	}
}
