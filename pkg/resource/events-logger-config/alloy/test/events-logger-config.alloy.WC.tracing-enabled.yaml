# This file was generated by logging-operator.
# It configures Alloy to be used as events logger.
# - configMap is generated from events-logger.alloy.template and passed as a string
#   here and will be created by Alloy's chart.
# - Alloy runs as a deployment, with only 1 replica.
alloy:
  alloy:
    configMap:
      create: true
      content: |-
        logging {
        	level  = "info"
        	format = "logfmt"
        }
        remote.kubernetes.secret "credentials" {
        	namespace = "kube-system"
        	name = "alloy-events"
        }
        loki.source.kubernetes_events "local" {
        	namespaces = []
        	forward_to = [loki.write.default.receiver]
        }
        // Loki target configuration
        loki.write "default" {
        	endpoint {
        		url                = nonsensitive(remote.kubernetes.secret.credentials.data["logging-url"])
        		max_backoff_period = "10m0s"
        		remote_timeout     = "1m0s"
        		tenant_id          = nonsensitive(remote.kubernetes.secret.credentials.data["logging-tenant-id"])
        		basic_auth {
        			username = nonsensitive(remote.kubernetes.secret.credentials.data["logging-username"])
        			password = remote.kubernetes.secret.credentials.data["logging-password"]
        		}
        		tls_config {
        			insecure_skip_verify = false
        		}
        	}
        	external_labels = {
        		cluster_id   = "test-cluster",
        		installation = "test-installation",
        		scrape_job = "kubernetes-events",
        	}
        }
        // OTLP receiver for traces
        otelcol.receiver.otlp "traces" {
        	grpc {
        		endpoint = "0.0.0.0:4317"
        		include_metadata = true
        	}
        	http {
        		endpoint = "0.0.0.0:4318"
        		include_metadata = true
        	}
        	output {
        		traces = [otelcol.processor.attributes.tenant_from_header.input]
        	}
        }
        // Extract tenant from X-Scope-OrgID header if present
        otelcol.processor.attributes "tenant_from_header" {
        	action {
        		key = "giantswarm.tenant_from_header"
        		action = "upsert"
        		from_attribute = "http.header.x-scope-orgid"
        	}
        	output {
        		traces = [otelcol.processor.k8sattributes.default.input]
        	}
        }
        otelcol.processor.k8sattributes "default" {
        	extract {
        		metadata = [
        			"k8s.namespace.name",
        			"k8s.pod.name",
        			"k8s.container.name",
        		]
        		label {
        			key = "observability.giantswarm.io/tenant"
        			tag_name = "giantswarm.tenant_from_label"
        		}
        		otel_annotations = true
        	}
        	output {
        		traces = [otelcol.processor.attributes.tenant_resolution.input]
        	}
        }
        // Resolve final tenant with priority: X-Scope-OrgID header -> pod label -> drop if none
        otelcol.processor.attributes "tenant_resolution" {
        	// Start with header value if present
        	action {
        		key = "giantswarm.tenant"
        		action = "upsert"
        		from_attribute = "giantswarm.tenant_from_header"
        	}
        	// If no tenant from header, use pod label (insert only sets if attribute doesn't exist)
        	action {
        		key = "giantswarm.tenant"
        		action = "insert"
        		from_attribute = "giantswarm.tenant_from_label"
        	}
        	// Clean up temporary attributes
        	action {
        		key = "giantswarm.tenant_from_header"
        		action = "delete"
        	}
        	action {
        		key = "giantswarm.tenant_from_label"
        		action = "delete"
        	}
        	output {
        		traces = [otelcol.processor.filter.tenant_gate.input]
        	}
        }
        // Drop traces that don't have a tenant assigned
        otelcol.processor.filter "tenant_gate" {
        	traces {
        		span = [
        			`resource.attributes["giantswarm.tenant"] == nil`,
        			`resource.attributes["giantswarm.tenant"] == ""`,
        		]
        	}
        	output {
        		traces = [
        			otelcol.processor.filter.giantswarm.input,
        		]
        	}
        }
        otelcol.processor.filter "giantswarm" {
        	traces {
        		span = [
        			`resource.attributes["giantswarm.tenant"] != "giantswarm"`,
        		]
        	}
        	output {
        		traces = [otelcol.processor.attributes.cleanup_tenant_giantswarm.input]
        	}
        }
        // Remove the giantswarm.tenant attribute before sending to exporter
        otelcol.processor.attributes "cleanup_tenant_giantswarm" {
        	action {
        		key = "giantswarm.tenant"
        		action = "delete"
        	}
        	output {
        		traces = [otelcol.exporter.otlphttp.giantswarm.input]
        	}
        }
        // one OTLP HTTP exporter for traces per tenant
        otelcol.exporter.otlphttp "giantswarm" {
        	client {
        		// TODO Try out /opentelemetry with grpc and not http
        		endpoint = "https://<tempo-url>"
        		headers = {
        			"X-Scope-OrgID" = "giantswarm",
        		}
        	}
        }
    # We decided to configure the alloy-events resources as such after some investigation done https://github.com/giantswarm/giantswarm/issues/32655
    resources:
      limits:
        cpu: 50m
        memory: 256Mi
      requests:
        cpu: 25m
        memory: 128Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: false
      runAsUser: 10
      runAsGroup: 10
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
  controller:
    type: deployment
    replicas: 1
  crds:
    create: false
  extraObjects:
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        meta.helm.sh/release-name: alloy-events
        meta.helm.sh/release-namespace: kube-system
      labels:
        app.kubernetes.io/component: networking
        app.kubernetes.io/instance: alloy-events
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: alloy
        app.kubernetes.io/part-of: alloy
        application.giantswarm.io/team: atlas
        giantswarm.io/managed-by: alloy-events
        giantswarm.io/service-type: managed
        helm.sh/chart: alloy-1.1.0
      name: otlp-gateway
      namespace: kube-system
    spec:
      ports:
      - appProtocol: grpc
        name: otlp
        port: 4317
        protocol: TCP
        targetPort: 4317
      - appProtocol: http
        name: otlp-http
        port: 4318
        protocol: TCP
        targetPort: 4318
      selector:
        app.kubernetes.io/instance: alloy-events
        app.kubernetes.io/name: alloy
      type: ClusterIP

networkPolicy:
  cilium:
    ingress:
    - toPorts:
      - ports:
        # Alloy Control Plane
        - port: "12345"
          protocol: TCP
        # OTLP GRPC
        - port: "4317"
          protocol: "TCP"
        # OTLP HTTP
        - port: "4318"
          protocol: "TCP"

verticalPodAutoscaler:
  enabled: true
  # We decided to configure the alloy-events vertical pod autoscaler as such after some investigation done https://github.com/giantswarm/giantswarm/issues/32655
  resourcePolicy:
    containerPolicies:
    - containerName: alloy
      controlledResources:
      - memory
      controlledValues: "RequestsAndLimits"
